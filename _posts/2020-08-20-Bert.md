---
layout: post
title: "Bert, kobert 강의내용 정리"
date: 2020-08-20
excerpt: "https://www.youtube.com/watch?v=qlxrXX5uBoU"
categories : [nlp]
tags: [nlp, bert]
comments: true
---



이 번 post는 soltlux 김성현 연구원 님의 강의를 요약한 내용입니다.

<https://www.youtube.com/watch?v=qlxrXX5uBoU>



## 자연어 처리

- 대화의 과정
메시지의 전달  --> 메시지의 부호화 --> 경로 --> 메시지의 수신 --> 메시지의 해독 --> 피드백
device 에서 data가 전달되는 방식역시 이와 닮아 있다.

규칙기반의 접근법:
    특정패턴을 만들어서 매핑을 시켜 사용하는 방법 
    ex) "국회 TV 에서 지금 뭐해?" ==> channel(now) ? program(what)
    ex) "지금 날시 어때?"         ==> outside(now) ? whether(what)
    현업에서 많이 사용하고 있는 방법

statistical approach ( 확률적 접근법 ):
    TF-IDF 를 이용한 키워드 추출
     * TF(term frequency) : 단어가 문서에 등장한 개수. 높을수록 중요한 단어
     * DF(document frequency) : 단어가 등장한 문서의 개수. 높을수록 중요하지 않은 단어

## 자연어 처리
* 문서분류
* 문법, 오타교정
* 정보추출
* 음성인식결과 보정 - soltlux 의 경우 AI 가 알아먹을 수 없는 단어 이므로 한글로 
  '솔' '트' '룩' '스' 라고 표기 혹은 발음해줌
* 음성합성 텍스트 보정
* 정보검색
* 요약문 생성
* 기계번역
* 질의응답
* 기계독해
* 챗봇
* 형태소분석
* 개체명분석
* 구문분석
* 감성분석
* 관계추출
* 의도파악

형태소분석, 문서분류, 개체명인식등 대부분이 분류의 문제
어떻게 분류 할 것인가?

분류 --> 분류 대상의 특징 추출

## one hot vec ==> sparserepresentation
문장을 어절단위로 분리하고 좌표위에 배치 --> n 개의 어절 발생 시 n개의 vector size 발생. 나열된 어절들이 unique 하지 않음.

## word2vec ==> dense representation
주변단어들을 통해 그 단어의 의미를 파악. 단어의 의미가 벡터로 표현됨으로써 벡터연산이 가능. 단 동형어 다의어 등에 대해서는 embedding 성능이 좋지 못하다.
ex) ( king vector - man vec ) + ( queen vec ) = women vec

## gensim 
word2vec 을 이용할 수 있는 라이브러리

## fasttext
facebook 에서 공개한 library. word2vec 과 동일하나 


## BERT 
기계독해로 가장 유명하다.

## OOV 
단어셋에 없는 단어를 뜻하며 indexing 이 불가능하다.





몇 번 더 보고 보충예정입니다.